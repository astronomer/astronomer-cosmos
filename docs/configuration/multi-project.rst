.. _multi-project:

Multi-Project Setups
====================

Cosmos supports multi-project dbt architectures where multiple dbt projects reference each other's models.
This is commonly achieved using `dbt-loom <https://github.com/nicholasyager/dbt-loom>`__, a dbt package
that enables cross-project references by injecting models from upstream projects into downstream projects.

This allows you to:

- Split large dbt projects into smaller, focused domain projects
- Share common staging models across multiple downstream projects
- Maintain clear boundaries between data domains while allowing references

Cosmos works with dbt-loom out of the box, automatically handling external node references.

How dbt-loom Works
------------------

dbt-loom enables cross-project references by:

1. Reading the ``manifest.json`` from upstream dbt projects
2. Injecting the upstream models' metadata into the downstream project's namespace
3. Allowing cross-project references using the syntax: ``{{ ref('upstream_project', 'model_name') }}``

How Cosmos Handles dbt-loom
---------------------------

When Cosmos parses a dbt project that uses dbt-loom, it encounters two types of nodes:

1. **Local nodes**: Models that exist as files in the current project
2. **External nodes**: Models injected by dbt-loom from upstream projects (no local file path)

Cosmos automatically:

- **Skips external nodes** during DAG generation (they don't have file paths)
- **Creates Airflow tasks only for local nodes** in each project
- **Maintains proper dependency tracking** within each project

This means you don't need any special configuration - Cosmos works with dbt-loom projects automatically.

Requirements
------------

For dbt-loom to work with Cosmos:

1. **For DAG parsing**: The upstream project's ``manifest.json`` must be accessible
2. **For task execution**: The downstream project must be able to query upstream tables

The upstream manifest can be generated by running any dbt command that parses the project:

.. code-block:: bash

    cd upstream_project
    dbt parse    # Fastest - just generates manifest
    # or
    dbt compile  # Also generates compiled SQL
    # or
    dbt ls       # Lists resources and generates manifest

Configuration Example
---------------------

Project Structure
~~~~~~~~~~~~~~~~~

A typical dbt-loom setup has an upstream project and one or more downstream projects:

.. code-block:: text

    dbt/
    ├── upstream_platform/           # Upstream project (staging, intermediate)
    │   ├── dbt_project.yml
    │   ├── profiles.yml
    │   ├── models/
    │   │   ├── staging/
    │   │   │   └── stg_customers.sql
    │   │   └── intermediate/
    │   │       └── int_customer_orders.sql
    │   └── target/
    │       └── manifest.json        # Required by downstream projects
    │
    └── downstream_finance/          # Downstream project (marts, reports)
        ├── dbt_project.yml
        ├── profiles.yml
        ├── dbt_loom.config.yml      # Points to upstream manifest
        ├── dependencies.yml         # Includes dbt-loom package
        └── models/
            └── fct_revenue.sql      # References upstream models

Upstream Project Configuration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The upstream project exposes models as ``public`` for cross-project access:

**dbt_project.yml**:

.. code-block:: yaml

    name: 'upstream_platform'
    version: '1.0.0'
    config-version: 2
    profile: 'upstream_platform'

    models:
      upstream_platform:
        staging:
          +materialized: view
          +access: public          # Required for dbt-loom
        intermediate:
          +materialized: view
          +access: public

Downstream Project Configuration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The downstream project configures dbt-loom to read from the upstream manifest:

**dependencies.yml**:

.. code-block:: yaml

    packages:
      - package: nicholasyager/dbt_loom
        version: ">=0.13.0"

    projects:
      - name: upstream_platform

**dbt_loom.config.yml**:

.. code-block:: yaml

    manifests:
      - name: upstream_platform
        type: file
        config:
          # Use environment variable for flexibility
          path: '{{ env_var("UPSTREAM_MANIFEST_PATH", "../upstream_platform/target/manifest.json") }}'

    enable_telemetry: false

**Model using cross-project ref**:

.. code-block:: sql

    -- fct_revenue.sql
    select
        c.customer_id,
        c.customer_name,
        sum(o.amount) as total_revenue
    from {{ ref('upstream_platform', 'stg_customers') }} c
    left join {{ ref('upstream_platform', 'int_customer_orders') }} o
        on c.customer_id = o.customer_id
    group by 1, 2

Cosmos DAG Configuration
~~~~~~~~~~~~~~~~~~~~~~~~

You can use either separate DAGs or a combined DAG with task groups:

**Option 1: Combined DAG with Task Groups (Recommended)**

.. code-block:: python

    from airflow import DAG
    from cosmos import (
        DbtTaskGroup,
        ProfileConfig,
        ProjectConfig,
        RenderConfig,
        ExecutionConfig,
    )
    from cosmos.profiles import PostgresUserPasswordProfileMapping
    from datetime import datetime
    from pathlib import Path

    DBT_ROOT = Path("/path/to/dbt")
    UPSTREAM_PATH = DBT_ROOT / "upstream_platform"
    DOWNSTREAM_PATH = DBT_ROOT / "downstream_finance"

    with DAG(
        dag_id="dbt_loom_dag",
        start_date=datetime(2024, 1, 1),
        schedule="@daily",
        catchup=False,
    ) as dag:

        # Upstream task group
        upstream = DbtTaskGroup(
            group_id="upstream_platform",
            project_config=ProjectConfig(dbt_project_path=UPSTREAM_PATH),
            profile_config=ProfileConfig(
                profile_name="upstream_platform",
                target_name="dev",
                profile_mapping=PostgresUserPasswordProfileMapping(
                    conn_id="postgres_conn",
                    profile_args={"schema": "platform"},
                ),
            ),
            execution_config=ExecutionConfig(dbt_project_path=UPSTREAM_PATH),
            render_config=RenderConfig(dbt_deps=True),
        )

        # Downstream task group with dbt-loom env vars
        dbt_loom_env = {
            "UPSTREAM_MANIFEST_PATH": str(UPSTREAM_PATH / "target" / "manifest.json"),
        }

        downstream = DbtTaskGroup(
            group_id="downstream_finance",
            project_config=ProjectConfig(dbt_project_path=DOWNSTREAM_PATH),
            profile_config=ProfileConfig(
                profile_name="downstream_finance",
                target_name="dev",
                profile_mapping=PostgresUserPasswordProfileMapping(
                    conn_id="postgres_conn",
                    profile_args={"schema": "finance"},
                ),
            ),
            execution_config=ExecutionConfig(dbt_project_path=DOWNSTREAM_PATH),
            render_config=RenderConfig(dbt_deps=True, env_vars=dbt_loom_env),
            operator_args={"env": dbt_loom_env},
        )

        # Chain: upstream runs first, then downstream
        upstream >> downstream

**Option 2: Separate DAGs with Datasets (Airflow 2.4+)**

.. code-block:: python

    from airflow.datasets import Dataset
    from cosmos import DbtDag, ProfileConfig, ProjectConfig

    PLATFORM_DATASET = Dataset("upstream_platform_complete")

    # Upstream DAG produces the dataset
    upstream_dag = DbtDag(
        dag_id="upstream_platform_dag",
        project_config=ProjectConfig(dbt_project_path=UPSTREAM_PATH),
        # ... other config ...
        schedule="@daily",
        outlets=[PLATFORM_DATASET],
    )

    # Downstream DAG triggers when dataset is updated
    downstream_dag = DbtDag(
        dag_id="downstream_finance_dag",
        project_config=ProjectConfig(dbt_project_path=DOWNSTREAM_PATH),
        # ... other config ...
        schedule=[PLATFORM_DATASET],  # Triggers on upstream completion
    )

Cross-Project Sources
---------------------

dbt-loom handles **model references** but does not directly support cross-project source references
(``{{ source('upstream_project', 'table') }}``). Here are the recommended patterns:

**Pattern 1: Wrap Sources in Staging Models (Recommended)**

Define sources in the upstream project and expose them via staging models:

.. code-block:: text

    upstream_platform/
    └── models/
        └── staging/
            ├── sources.yml          # Source definition
            └── stg_raw_orders.sql   # Staging model wrapping the source

**sources.yml** (upstream):

.. code-block:: yaml

    version: 2
    sources:
      - name: raw_data
        schema: raw
        tables:
          - name: orders

**stg_raw_orders.sql** (upstream):

.. code-block:: sql

    {{ config(materialized='view', access='public') }}

    select * from {{ source('raw_data', 'orders') }}

Now the downstream project references the staging model instead of the source:

.. code-block:: sql

    -- downstream model
    select * from {{ ref('upstream_platform', 'stg_raw_orders') }}

**Pattern 2: Duplicate Source Definitions**

If you must reference the same raw table in multiple projects, define the source in each project:

.. code-block:: yaml

    # In both upstream and downstream projects
    version: 2
    sources:
      - name: shared_raw_data
        database: "{{ env_var('RAW_DATABASE') }}"
        schema: raw
        tables:
          - name: orders

This approach requires keeping source definitions in sync across projects.


Cross-Project Macros
--------------------

dbt-loom does **not** handle macro sharing. Macros are resolved at compile time within each project.
Here are the recommended patterns for sharing macros:

**Pattern 1: Create a Shared dbt Package (Recommended)**

Create a separate dbt package containing shared macros and install it in all projects:

.. code-block:: text

    shared_macros/                    # Shared package (separate repo)
    ├── dbt_project.yml
    └── macros/
        ├── generate_schema_name.sql
        ├── cents_to_dollars.sql
        └── hash_columns.sql

**dbt_project.yml** (shared package):

.. code-block:: yaml

    name: 'company_shared_macros'
    version: '1.0.0'
    config-version: 2

Install in each project via **packages.yml** or **dependencies.yml**:

.. code-block:: yaml

    packages:
      # From git repository
      - git: "https://github.com/your-org/company-shared-macros.git"
        revision: v1.0.0

      # Or from local path (for development)
      - local: ../shared_macros

Use the macro with the package prefix:

.. code-block:: sql

    select
        {{ company_shared_macros.cents_to_dollars('amount_cents') }} as amount_dollars
    from {{ ref('orders') }}

**Pattern 2: Copy Macros to Each Project**

For simpler setups, copy commonly used macros to each project. This is easier to maintain
for a small number of macros but doesn't scale well.

**Pattern 3: Override dbt Built-in Macros Consistently**

If you override dbt built-in macros (like ``generate_schema_name``), ensure the override
is consistent across all projects:

.. code-block:: sql

    -- macros/generate_schema_name.sql (same in all projects)
    {% macro generate_schema_name(custom_schema_name, node) %}
        {% if custom_schema_name %}
            {{ custom_schema_name }}
        {% else %}
            {{ target.schema }}
        {% endif %}
    {% endmacro %}

**Macro Sharing Summary**

.. list-table::
   :widths: 30 35 35
   :header-rows: 1

   * - Approach
     - Pros
     - Cons
   * - Shared dbt Package
     - Single source of truth, versioned
     - Requires package management setup
   * - Copy Macros
     - Simple, no dependencies
     - Hard to keep in sync
   * - Consistent Overrides
     - Works for built-in macros
     - Limited to override scenarios

Troubleshooting
---------------

**Error: "The path does not exist" for manifest.json**

This occurs when dbt-loom can't find the upstream manifest. Solutions:

1. Use an absolute path in ``dbt_loom.config.yml``
2. Set the ``UPSTREAM_MANIFEST_PATH`` environment variable
3. Ensure the upstream project has been parsed (run ``dbt parse``)

**Error: "unsupported operand type(s) for /: 'PosixPath' and 'NoneType'"**

This occurred in older Cosmos versions when external nodes (from dbt-loom) didn't have file paths.
This is now fixed - Cosmos 1.13.0+ automatically skips nodes without file paths.

**Error: "Table does not exist" during execution**

The upstream tables must exist in the database before running downstream models:

1. Ensure the upstream project has been executed (not just parsed)
2. Verify both projects can access the same database/schemas
3. Check that cross-database access is configured if using different databases

Best Practices
--------------

1. **Use environment variables** for manifest paths to support different environments
2. **Chain task groups** or use datasets to ensure proper execution order
3. **Mark upstream models as public** using ``+access: public``
4. **Generate manifests in CI** to ensure they're always available
5. **Use persistent storage** (not in-memory databases) for cross-project data sharing

Limitations
-----------

- dbt-loom external nodes are skipped during Cosmos DAG generation (by design)
- Cross-project lineage is not yet visualized in Airflow's lineage view
